{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b07ZL26hrEy9"
      },
      "source": [
        "# Fine-tune a Large Language Model with LoRA\n",
        "\n",
        "This is a part of Lab 6 of the [EE292D Edge ML class](https://ee292d.github.io/) at Stanford, which covers parameter-efficient fine-tuning and deployment of LLMs.\n",
        "\n",
        "You'll need a GPU for this exercise. As with previous labs, you can access them for free on Colab. [Click here](https://colab.research.google.com/github/ee292d/labs/blob/main/lab6/notebook.ipynb) to open this notebook in a Colab instance, then change your runtime type to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqAcEFDE9o1r"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Our goal is to fine-tune a small large language model (LLM) for a new task, then prepare it for deployment on a Raspberry Pi. In this example, we will fine-tune a base model that has been pre-trained for _completion_ (i.e., to predict the next words in the input sentence) so that we can use it for _chat_.\n",
        "\n",
        "We're going to fine-tune using a technique called \"low rank adaptation\" (LoRA). Vanilla fine-tuning of LLMs requires a massive amount of GPU memory because we are directly updating the weights of the model during training. With LoRA, we train a small _adapter layer_ rather than retraining the whole model. Once we're done, we merge this small adapter layer into the original model to get our fine-tuned model .\n",
        "\n",
        "To get started, install the required Python dependencies in your environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW-1z4dD6dxO",
        "outputId": "f8b71b42-263b-4a29-cdf1-ac3733bcfbaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops~=0.7.0 (from -r requirements.txt (line 1))\n",
            "  Obtaining dependency information for einops~=0.7.0 from https://files.pythonhosted.org/packages/29/0b/2d1c0ebfd092e25935b86509a9a817159212d82aa43d7fb07eca4eeff2c2/einops-0.7.0-py3-none-any.whl.metadata\n",
            "  Using cached einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: accelerate==0.29.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (0.29.2)\n",
            "Requirement already satisfied: aiohttp==3.9.4 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (3.9.4)\n",
            "Requirement already satisfied: aiosignal==1.3.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: asttokens==2.4.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (2.4.1)\n",
            "Requirement already satisfied: attrs==23.2.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (23.2.0)\n",
            "Requirement already satisfied: certifi==2024.2.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (2024.2.2)\n",
            "Requirement already satisfied: charset-normalizer==3.3.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (3.3.2)\n",
            "Requirement already satisfied: comm==0.2.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (0.2.2)\n",
            "Requirement already satisfied: datasets==2.18.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (2.18.0)\n",
            "Requirement already satisfied: debugpy==1.8.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (1.8.1)\n",
            "Requirement already satisfied: decorator==5.1.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (5.1.1)\n",
            "Requirement already satisfied: dill==0.3.8 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (0.3.8)\n",
            "Requirement already satisfied: docstring_parser==0.16 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (0.16)\n",
            "Requirement already satisfied: executing==2.0.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (2.0.1)\n",
            "Requirement already satisfied: filelock==3.13.4 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (3.13.4)\n",
            "Requirement already satisfied: frozenlist==1.4.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (1.4.1)\n",
            "Requirement already satisfied: fsspec==2024.2.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (2024.2.0)\n",
            "Requirement already satisfied: huggingface-hub==0.22.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (0.22.2)\n",
            "Requirement already satisfied: idna==3.7 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 20)) (3.7)\n",
            "Requirement already satisfied: ipykernel==6.29.4 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 21)) (6.29.4)\n",
            "Requirement already satisfied: ipython==8.23.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 22)) (8.23.0)\n",
            "Requirement already satisfied: jedi==0.19.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 23)) (0.19.1)\n",
            "Requirement already satisfied: Jinja2==3.1.3 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 24)) (3.1.3)\n",
            "Requirement already satisfied: jupyter_client==8.6.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 25)) (8.6.1)\n",
            "Requirement already satisfied: jupyter_core==5.7.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 26)) (5.7.2)\n",
            "Requirement already satisfied: markdown-it-py==3.0.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 27)) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe==2.1.5 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 28)) (2.1.5)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.7 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 29)) (0.1.7)\n",
            "Requirement already satisfied: mdurl==0.1.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 30)) (0.1.2)\n",
            "Requirement already satisfied: mpmath==1.3.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 31)) (1.3.0)\n",
            "Requirement already satisfied: multidict==6.0.5 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 32)) (6.0.5)\n",
            "Requirement already satisfied: multiprocess==0.70.16 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 33)) (0.70.16)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 34)) (1.6.0)\n",
            "Requirement already satisfied: networkx==3.3 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 35)) (3.3)\n",
            "Requirement already satisfied: numpy==1.26.4 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 36)) (1.26.4)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 37)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 38)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 39)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 40)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 41)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 42)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 43)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 44)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 45)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 46)) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 47)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 48)) (12.1.105)\n",
            "Requirement already satisfied: packaging==24.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 49)) (24.0)\n",
            "Requirement already satisfied: pandas==2.2.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 50)) (2.2.2)\n",
            "Requirement already satisfied: parso==0.8.4 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 51)) (0.8.4)\n",
            "Requirement already satisfied: peft==0.10.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 52)) (0.10.0)\n",
            "Requirement already satisfied: pexpect==4.9.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 53)) (4.9.0)\n",
            "Requirement already satisfied: platformdirs==4.2.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 54)) (4.2.0)\n",
            "Requirement already satisfied: prompt-toolkit==3.0.43 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 55)) (3.0.43)\n",
            "Requirement already satisfied: psutil==5.9.8 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 56)) (5.9.8)\n",
            "Requirement already satisfied: ptyprocess==0.7.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 57)) (0.7.0)\n",
            "Requirement already satisfied: pure-eval==0.2.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 58)) (0.2.2)\n",
            "Requirement already satisfied: pyarrow==15.0.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 59)) (15.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix==0.6 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 60)) (0.6)\n",
            "Requirement already satisfied: Pygments==2.17.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 61)) (2.17.2)\n",
            "Requirement already satisfied: python-dateutil==2.9.0.post0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 62)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz==2024.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 63)) (2024.1)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 64)) (6.0.1)\n",
            "Requirement already satisfied: pyzmq==26.0.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 65)) (26.0.0)\n",
            "Requirement already satisfied: regex==2023.12.25 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 66)) (2023.12.25)\n",
            "Requirement already satisfied: requests==2.31.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 67)) (2.31.0)\n",
            "Requirement already satisfied: rich==13.7.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 68)) (13.7.1)\n",
            "Requirement already satisfied: safetensors==0.4.3 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 69)) (0.4.3)\n",
            "Requirement already satisfied: shtab==1.7.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 70)) (1.7.1)\n",
            "Requirement already satisfied: six==1.16.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 71)) (1.16.0)\n",
            "Requirement already satisfied: stack-data==0.6.3 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 72)) (0.6.3)\n",
            "Requirement already satisfied: sympy==1.12 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 73)) (1.12)\n",
            "Requirement already satisfied: tokenizers==0.15.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 74)) (0.15.2)\n",
            "Requirement already satisfied: torch==2.2.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 75)) (2.2.2)\n",
            "Requirement already satisfied: tornado==6.4 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 76)) (6.4)\n",
            "Requirement already satisfied: tqdm==4.66.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 77)) (4.66.2)\n",
            "Requirement already satisfied: traitlets==5.14.2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 78)) (5.14.2)\n",
            "Requirement already satisfied: transformers==4.39.3 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 79)) (4.39.3)\n",
            "Requirement already satisfied: triton==2.2.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 80)) (2.2.0)\n",
            "Requirement already satisfied: trl==0.8.3 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 81)) (0.8.3)\n",
            "Requirement already satisfied: typing_extensions==4.11.0 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 82)) (4.11.0)\n",
            "Requirement already satisfied: tyro==0.8.3 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 83)) (0.8.3)\n",
            "Requirement already satisfied: tzdata==2024.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 84)) (2024.1)\n",
            "Requirement already satisfied: urllib3==2.2.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 85)) (2.2.1)\n",
            "Requirement already satisfied: wcwidth==0.2.13 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 86)) (0.2.13)\n",
            "Requirement already satisfied: xxhash==3.4.1 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 87)) (3.4.1)\n",
            "Requirement already satisfied: yarl==1.9.4 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 88)) (1.9.4)\n",
            "Collecting sentencepiece~=0.1.98 (from -r requirements.txt (line 89))\n",
            "  Obtaining dependency information for sentencepiece~=0.1.98 from https://files.pythonhosted.org/packages/4d/9d/9153942f0e2143a43978bcefba31d79187b7037bed3f85a6668c69493062/sentencepiece-0.1.99-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Using cached sentencepiece-0.1.99-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting gguf>=0.1.0 (from -r requirements.txt (line 90))\n",
            "  Obtaining dependency information for gguf>=0.1.0 from https://files.pythonhosted.org/packages/97/a4/83969343abb00fe787de5965c5c1f617aa51b2e2c563d4391c402aba548f/gguf-0.6.0-py3-none-any.whl.metadata\n",
            "  Downloading gguf-0.6.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r requirements.txt (line 91))\n",
            "  Obtaining dependency information for protobuf<5.0.0,>=4.21.0 from https://files.pythonhosted.org/packages/15/db/7f731524fe0e56c6b2eb57d05b55d3badd80ef7d1f1ed59db191b2fdd8ab/protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
            "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cached einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "Using cached sentencepiece-0.1.99-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Downloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
            "Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Installing collected packages: sentencepiece, protobuf, gguf, einops\n",
            "Successfully installed einops-0.7.0 gguf-0.6.0 protobuf-4.25.3 sentencepiece-0.1.99\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LkaVXNt6XuH"
      },
      "source": [
        "## Choosing a Base Model\n",
        "\n",
        "We'll work with a lightweight base model: [Phi-2](https://huggingface.co/microsoft/phi-2). At 2.7B parameters, Phi-2 can fit in about 5GB of RAM when loaded at 16-bit precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NJJx6pJvqrLz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/paperspace/useful/labs/lab6/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.64it/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "   \"microsoft/phi-2\",\n",
        "   torch_dtype=torch.bfloat16,\n",
        "   trust_remote_code=True\n",
        ").to(\"cuda\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"microsoft/phi-2\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi9tkfAz2gV6"
      },
      "source": [
        "Now that we have the model loaded, we can try an input:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "zj7lDgFyx7yF",
        "outputId": "d1a3de09-dcec-4bad-f3f2-0cc4b05fffbe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Hello Phi!\\n\\nI am writing to you today to share some exciting news about the Phi Beta Kappa Society. As you may know, Phi Beta Kappa is a prestigious honor society that recognizes academic excellence and leadership in the United States. It was founded in 1776 and has since grown to include over 1,000 chapters in colleges and universities across the country.\\n\\nRecently, the Phi Beta Kappa Society has been making some changes to its membership requirements. In the past, membership was limited to students who had completed their undergraduate studies and were pursuing a graduate degree. However, in recent years, the society has expanded its membership to include students who have completed their undergraduate studies and are pursuing a professional degree. This means that more students have the opportunity to be recognized for their academic achievements and leadership skills.\\n\\nIn addition to these changes, the Phi Beta Kappa Society has also been working to increase diversity among its members. In the past, the society has been criticized for its lack of diversity, with'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def prompt_phi(text):\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    inputs.to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(**inputs, max_length=200)\n",
        "    return tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "prompt_phi(\"Hello Phi!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_MM_Hsd3u3K"
      },
      "source": [
        "We want to chat with Phi-2, but at the moment it's just completing our sentences. We'll have to teach it some conversation skills with fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYHQYVf_xAOO"
      },
      "source": [
        "## Preparing a Fine-tuning Dataset\n",
        "\n",
        "When we fine-tune a model, we show it a large set of examples of what we want the model's outputs to look like. For example, if we wanted to tune a generative image model to produce images in the style of Garfield comics, we would fine-tune it on a large set of comics. Under the hood, we are nudging the weights of the model so that the probability distribution of the output shifts toward the distribution observed in our set of training examples.\n",
        "\n",
        "In this exercise, we want our model's outputs to look like a conversation between a human and a chatbot. Our fine-tuning dataset will accordingly consist of a set of examples of conversations. There are a few key things we want the model to learn from these examples:\n",
        "\n",
        "- **Structure:** We saw that our prompt to the base model earlier didn't result in a conversational response, but rather a rambling _completion_ of the input text. Our examples need to teach the model the structure of a conversation: a human and assistant taking turns responding to one another. We will demarcate this by `### Human:` and `### Assistant:` tags. That way, when we prompt the model with text like `### Human: Hey, how's it going today?` the model knows what comes next is a response, in the structure `### Assistant: ...`.\n",
        "\n",
        "- **Tone:** Beyond structure, we are also teaching the model to adhere to a particular tone. The tone and content of conversations in our training set will be reflected in the fine-tuned model's outputs. If we want the chatbot to take on a particular persona, or be conversant in a particular domain, we need to ensure our training set exemplifies that.\n",
        "\n",
        "There are several datasets for chat, e.g., [OpenAssistant](https://huggingface.co/datasets/timdettmers/openassistant-guanaco), or [LMSys Chat](https://huggingface.co/datasets/lmsys/lmsys-chat-1m). Our Phi-2 base model is pre-trained on English, so we our fine-tuning set needs to be in English as well. For this exercise, we'll use a small English subset of the OpenAssistant dataset. \n",
        "\n",
        "First, we'll load it and select a subset: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ1YBuVU6H9d",
        "outputId": "f5429125-e48b-4baf-b87c-793aed4f4e91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['messages'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"sablo/oasst2_curated\", split=\"train\").select(range(1000))\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we need to preprocess the data. Let's see what the raw format is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luBE5aJU63Zd",
        "outputId": "b3587519-e67a-4219-c232-bdfac62ad059"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': 'I am making mayonnaise, it was starting to thicken but now it has become runny and liquid again, is there any way to salvage it?',\n",
              "  'role': 'user'},\n",
              " {'content': \"Yes, it's possible to fix runny mayonnaise! The most common reason for mayonnaise becoming runny is because the oil was added too quickly or the egg yolk wasn't emulsified properly. Here are some steps you can take to fix it:\\n\\n1. Separate another egg yolk and place it in a clean, dry bowl.\\n2. Slowly add the runny mayonnaise to the egg yolk while whisking vigorously.\\n3. Once all the runny mayonnaise has been added, continue whisking until the mixture has emulsified and thickened.\\n4. If the mayonnaise is still too runny, you can add another egg yolk and repeat the process.\\n\\nIf the mayonnaise still won't thicken, you can try adding a small amount of dijon mustard or vinegar to the mixture, which can act as emulsifiers and help stabilize the mayonnaise. It's important to add these ingredients slowly and in small amounts to avoid over-thinning the mixture.\",\n",
              "  'role': 'assistant'},\n",
              " {'content': 'What is optimal Mayonnaise thickness?', 'role': 'user'},\n",
              " {'content': 'The optimal mayonnaise thickness will depend on how it is being used. A runny mayonnaise may be good in chicken salad while a thicker mayonnaise may be better spread over a hamburger bun. The only way to determine your personal preference is to test different levels of viscosity in with different foods.',\n",
              "  'role': 'assistant'}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = dataset[0]\n",
        "example[\"messages\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each item in the raw dataset is a list of dictionaries, where each dictionary is a turn in the conversation (`user` or `assistant`). We need to turn each item into a training example with our desired format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Human: I am making mayonnaise, it was starting to thicken but now it has become runny and liquid again, is there any way to salvage it?\n",
            "### Assistant: Yes, it's possible to fix runny mayonnaise! The most common reason for mayonnaise becoming runny is because the oil was added too quickly or the egg yolk wasn't emulsified properly. Here are some steps you can take to fix it:\n",
            "\n",
            "1. Separate another egg yolk and place it in a clean, dry bowl.\n",
            "2. Slowly add the runny mayonnaise to the egg yolk while whisking vigorously.\n",
            "3. Once all the runny mayonnaise has been added, continue whisking until the mixture has emulsified and thickened.\n",
            "4. If the mayonnaise is still too runny, you can add another egg yolk and repeat the process.\n",
            "\n",
            "If the mayonnaise still won't thicken, you can try adding a small amount of dijon mustard or vinegar to the mixture, which can act as emulsifiers and help stabilize the mayonnaise. It's important to add these ingredients slowly and in small amounts to avoid over-thinning the mixture.\n",
            "### Human: What is optimal Mayonnaise thickness?\n",
            "### Assistant: The optimal mayonnaise thickness will depend on how it is being used. A runny mayonnaise may be good in chicken salad while a thicker mayonnaise may be better spread over a hamburger bun. The only way to determine your personal preference is to test different levels of viscosity in with different foods.\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "def format_example(example):\n",
        "    messages = example[\"messages\"]\n",
        "    training_instance = \"\"\n",
        "    for turn in messages:\n",
        "        if turn[\"role\"] == \"user\":\n",
        "            training_instance += f\"### Human: {turn['content']}\\n\"\n",
        "        elif turn[\"role\"] == \"assistant\":\n",
        "            training_instance += f\"### Assistant: {turn['content']}\\n\"\n",
        "    training_instance += tokenizer.eos_token\n",
        "    return training_instance\n",
        "\n",
        "print(format_example(example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we'll map this function to the dataset to create a column of formatted training examples, then create an 80/20 train/test split for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 12484.79 examples/s]\n"
          ]
        }
      ],
      "source": [
        "dataset = dataset.map(lambda x: {\"training_example\": format_example(x)})\n",
        "splits = dataset.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkckxpDz6LML"
      },
      "source": [
        "## Fine-tuning\n",
        "\n",
        "To fine-tune with LoRA, we'll use the Huggingface PEFT (Parameter Efficient Fine Tuning) library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4Pp_klBy7BTF",
        "outputId": "c24a2aa6-bd86-4beb-d888-4fc4c309c066"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 800/800 [00:00<00:00, 3866.64 examples/s]\n",
            "Map: 100%|██████████| 200/200 [00:00<00:00, 3756.20 examples/s]\n",
            "/home/paperspace/useful/labs/lab6/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1200/1200 20:24, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.359900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.353200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.322300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.357100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.320700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.295300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.286300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.341900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.328600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.292100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.246200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.305400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1200, training_loss=1.3174248441060383, metrics={'train_runtime': 1226.4031, 'train_samples_per_second': 1.957, 'train_steps_per_second': 0.978, 'total_flos': 2.214744389351424e+16, 'train_loss': 1.3174248441060383, 'epoch': 3.0})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "from peft import get_peft_model, LoraConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "output_dir = './phi-2-chat/'\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=8,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# wrap the model for fine-tuning with lora\n",
        "lora_model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    save_strategy='epoch',\n",
        "    logging_steps=100,\n",
        "    per_device_eval_batch_size=2,   # keep the batch size small so we don't run out of GPU memory\n",
        "    per_device_train_batch_size=2,\n",
        "    load_best_model_at_end=True,\n",
        "    save_strategy = \"no\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=lora_model,\n",
        "    train_dataset=splits[\"train\"],               \n",
        "    eval_dataset=splits[\"test\"],\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"training_example\",\n",
        "    max_seq_length=2048,            # the context window of phi-2 is 2048 tokens, so we set this as the max during fine-tuning\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we're ready to fine-tune the model. We're using a relatively small dataset, which should limit the training time to about 20 minutes. Note: if we weren't using gradient checkpointing, this would run much faster. However, we need to use gradient checkpointing to keep the memory requirements of training low enough for Colab free instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "trainer.save_model(f\"{output_dir}/adapter-layer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try a prompt to the fine-tuned model. We need to prepend the `### Human: ` tag, since this is the prompt structure that we fine-tuned the model on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/home/paperspace/useful/labs/lab6/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'### Human: Hello Phi!\\n### Assistant: Hello! How can I assist you today?\\n### Human: I am a human and I am curious about the concept of the golden ratio. Can you explain it to me in simple terms?\\n### Assistant: Sure! The golden ratio is a mathematical concept that has been used in art, architecture, and design for centuries. It is a ratio of two quantities that is equal to the ratio of their sum to the larger of the two quantities. The golden ratio is approximately 1.618.\\n\\nThe golden ratio is often represented by the Greek letter phi (φ). It is believed to be a divine proportion, and has been used in art and architecture to create aesthetically pleasing compositions.\\n\\nThe golden ratio can be found in many natural phenomena, such as the spiral of a seashell or the branching of a tree. It is also used in computer graphics and animation to create realistic-looking images.\\n\\nIn summary,'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def prompt_phi_finetune(text):\n",
        "    return prompt_phi(f\"### Human: {text}\\n\")\n",
        "\n",
        "prompt_phi_finetune(\"Hello Phi!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You'll notice two things:\n",
        "\n",
        "1. Our outputs are now adhering to the turn-by-turn `### Human:` `### Assistant:` structure in our training examples!\n",
        "2. The model continues the conversation past the first response.\n",
        "\n",
        "To the second point, this is because we only fine-tuned the model on a small number of examples, so it hasn't seen enough data to learn when its response should stop. Even though this is the case, there is enough structure in the text to extract the chat responses. Let's do that now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hey Phi, how's it going?\n",
            "Phi: I'm doing well, thank you! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def extract_response(text):\n",
        "    \"\"\" We know where to extract the chat response since we fine-tuned \n",
        "    the model to use the \"### Assistant:\"\" tag. \"\"\"\n",
        "    pattern = r\"### Assistant:(.*?)\\n\"\n",
        "    match = re.search(pattern, text)\n",
        "\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def phi_chat(message):\n",
        "    print(f\"User: {message}\")\n",
        "    raw_response = prompt_phi_finetune(message)\n",
        "    print(f\"Phi: {extract_response(raw_response)}\")\n",
        "\n",
        "phi_chat(\"Hey Phi, how's it going?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to create a multi-turn conversational interface, we simply chain together each turn in the conversation, providing Phi-2 the full \"context\" of the conversation each time we prompt it (up to its token limit of 2048). Our inputs should always have the same structure as our training examples. This is out of scope for this exercise, but you can imagine how we'd implement this. Give it a shot if you like!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQQrYAJj6NcV"
      },
      "source": [
        "## Merging Weights\n",
        "\n",
        "Recall that LoRA trains an _adapter layer_, rather than a full model. If you look at the filesize of the weights we trained, this becomes apparent. While the Phi-2 model is about 5GB, the adapter that we trained is only about 30MB. In order to get the model ready for deployment on our Pi, we need to merge the adapter layer back into the weights of the base model. We'll do this now.\n",
        "\n",
        "Note: **you'll need to restart the runtime before running these cells.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/paperspace/useful/labs/lab6/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "   \"microsoft/phi-2\",\n",
        "   torch_dtype=torch.bfloat16,\n",
        "   trust_remote_code=True\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "copy tokenizer_config.json to ./phi-2-ft/final\n",
            "copy special_tokens_map.json to ./phi-2-ft/final\n",
            "copy added_tokens.json to ./phi-2-ft/final\n",
            "copy tokenizer.json to ./phi-2-ft/final\n"
          ]
        }
      ],
      "source": [
        "from peft import PeftModel\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "adapter_dir = './phi-2-chat/adapter-layer'\n",
        "output_dir = './phi-2-chat/merged-weights'\n",
        "\n",
        "# this loads a base model with adapter layers attached. \n",
        "# note: the first argument is a MODEL, while the second is a PATH TO A MODEL.\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    adapter_dir\n",
        ")\n",
        "\n",
        "model = model.merge_and_unload() # merge adapters with the base model\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# save the merged weights. does not save any of the (important) tokenizer metadata...\n",
        "model.save_pretrained(output_dir)\n",
        "\n",
        "# ...so let's copy it over\n",
        "for f in os.listdir(adapter_dir):\n",
        "    if 'token' in f:\n",
        "        print(f'copy {f} to {output_dir}')\n",
        "        shutil.copyfile(os.path.join(adapter_dir, f), os.path.join(output_dir, f))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You've now fine-tuned a small base model for a new use case, and prepared the merged weights for deployment/distribution. There's one step left to get them running on the Raspberry Pi, however: you'll need to convert them to the right format for the [llama.cpp](https://github.com/ggerganov/llama.cpp) framework. Let's get the framework (which includes the necessary conversion scripts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "Warning: Permanently added the ECDSA host key for IP address '140.82.116.4' to the list of known hosts.\n",
            "remote: Enumerating objects: 22357, done.\u001b[K\n",
            "remote: Counting objects: 100% (5995/5995), done.\u001b[K\n",
            "remote: Compressing objects: 100% (343/343), done.\u001b[K\n",
            "remote: Total 22357 (delta 5823), reused 5708 (delta 5652), pack-reused 16362\u001b[K\n",
            "Receiving objects: 100% (22357/22357), 26.90 MiB | 23.18 MiB/s, done.\n",
            "Resolving deltas: 100% (15826/15826), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone git@github.com:ggerganov/llama.cpp.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we use llama.cpp's conversion scripts to convert our merged weights (in HuggingFace format) to llama.cpp's format (GGUF)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: merged-weights\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "Set model parameters\n",
            "Set model tokenizer\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "gguf: Adding 50000 merge(s).\n",
            "gguf: Setting special token type bos to 50256\n",
            "gguf: Setting special token type eos to 50256\n",
            "gguf: Setting special token type unk to 50256\n",
            "gguf: Setting special token type pad to 50256\n",
            "Exporting model to 'phi-2-chat/phi-2-chat.gguf'\n",
            "gguf: loading model part 'model-00001-of-00002.safetensors'\n",
            "token_embd.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.24.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.24.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.24.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.24.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.24.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.24.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.24.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.24.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.24.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.24.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.24.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.24.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.24.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.24.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.25.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.25.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.25.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.25.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.25.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.25.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.25.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.25.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.25.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.25.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.25.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.25.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.25.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.25.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.26.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.26.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.26.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.26.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.26.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.26.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.26.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.26.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.26.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.26.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.26.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.26.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.26.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.26.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.27.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.27.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.27.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.27.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.27.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.27.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.27.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.27.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.27.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.27.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.27.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.27.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.27.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.27.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.28.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.28.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.28.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.28.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.28.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.28.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.28.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.28.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.28.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.28.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.28.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.28.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.28.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.28.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.29.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.29.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.29.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.29.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.29.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.29.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.29.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.29.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.29.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.29.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.29.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.29.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.29.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.29.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.30.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.30.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "gguf: loading model part 'model-00002-of-00002.safetensors'\n",
            "output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "output_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "output_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.30.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.30.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.30.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.30.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.30.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.30.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.30.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.30.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.30.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.30.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.30.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.30.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.31.attn_norm.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.31.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.31.ffn_up.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.31.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.31.ffn_down.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.31.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.31.attn_output.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.31.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.31.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.31.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.31.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.31.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.31.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.31.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "Model successfully exported to 'phi-2-chat/phi-2-chat.gguf'\n"
          ]
        }
      ],
      "source": [
        "!python llama.cpp/convert-hf-to-gguf.py phi-2-chat/merged-weights --outfile phi-2-chat/phi-2-chat.gguf --outtype f16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And that's it! Make sure you download the `phi-2-chat.gguf` weights before disconnecting from your Colab instance. You can now transfer the GGUF weights to your Raspberry Pi and use them with the example code to build your own fully-local chat application."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
